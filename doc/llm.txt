*llm.txt*                                                               llm.vim

AI Assistant for Vim                                    *llm-plugin*

CONTENTS                                                *llm-contents*
  1. Introduction.......................|llm-introduction|
  2. Installation.......................|llm-installation|
  3. Keymaps............................|llm-keymaps|
  4. Commands...........................|llm-commands|
  5. Usage..............................|llm-usage|
  5. Options............................|llm-options|
  6. Notes..............................|llm-notes|

==============================================================================
1. Introduction                                         *llm-introduction*

llm.vim is a Vim plugin that integrates with Simon Willsion's `llm`
command-line tool to provide an AI chat assistant directly within Vim.
It manages a dedicated chat buffer, sends prompts to the `llm` tool, and
streams responses back into the buffer, allowing for an interactive
AI experience without laving your editor.

==============================================================================
2. Installation                                         *llm-installation*

This plugin requires the external `llm` command-line tool to be available in
your system's PATH and already configured.

Install using a plugin manager like `vim-plug`:
>
    Plug 'cbagwell/llm.vim'
<
Then run `:PlugInstall`.

==============================================================================
3. Keymaps                                              *llm-keymaps*

The following keymaps are active when in the `llmchat` buffer:

<CR>                                                    *<CR>*
    Sends the current prompt in the `llmchat` buffer to the `llm` tool,
    effectively calling `:LLMChat`.

<C-c>                                                   *<C-c>*
    Cancels any currently running `llm` job, equivalent to calling
    `:LLMCancel`.

==============================================================================
4. Commands                                             *llm-commands*

                                                        *LLMChat*
:LLMChat [args]
:{range}LLMChat [args]
:{Visual}LLMChat [args]
    Opens or switches to the `llmchat` buffer and interacts with the `llm`
    tool.

    - If `[args]` are provided, they are treated as a new "User Prompt" and
      sent to the `llm` tool immediately.
    - If no `[args]` are provided:
        - If the last entry in the `llmchat` buffer is a "User Prompt",
          buffer content is sent to the `llm` tool.
        - Otherwise, a new empty "User Prompt" header is added, and the
          cursor is placed below it, allowing you to type your prompt before
	  re-issuing the command.
          You then call `:LLMChat` again (without arguments) to send it.
    - If a visual selection is active when `:LLMChat` is called,
      the selected lines are appended to the `llmchat` buffer under a
      Pasted Prompt header. This content is included in the context
      sent to the `llm` tool if a prompt is sent immediately after.

                                                        *LLMCancel*
:LLMCancel
    Cancels any currently running `llm` job. If no job is running, it will
    display a message.

                                                        *LLMIsRunning*
:LLMIsRunning
    Echoes "1" if an `llm` job is currently running, "0" otherwise.

                                                        *LLMFix*
:LLMFix
:{range}LLMFix
:{Visual}LLMFix
    Sends the current visual selection/range/line to LLM to fix any
    syntactical errors and replaces with the results.
                                                        *LLMFilter*
:LLMFilter
:{range}LLMFilter
:{Visual}LLMFilter
    Sends the current visual selection/range/line to `llm` to filter or
    transform it. The prompt given to `:LLMFilter` guides the transformation.
    For example: `:'<,'>LLMFilter Improve this text. Return only the original
    text with improvements.`

                                                        *LLMComplete*
:LLMComplete
:{range}LLMComplete
:{Visual}LLMComplete
    Sends the current visual selection/range/line to `llm` to complete it.
    Can be used both for implement a function based on comment description
    or to complete incomplete sentences/paragraphs.

==============================================================================
4. Usage                                                *llm-usage*

Start a new chat or continue an existing one:
>
    :LLMChat
<
    This will open the `llmchat` buffer. If it's a new chat or the last
    response was from `llm`, it will add a new "User Prompt" header.
    You can then type your prompt and press `<CR>` (or move off the line)
    and call `:LLMChat` again to send it.

Send a direct prompt:
>
    :LLMChat What is the capital of France?
<
    This will immediately send "What is the capital of France?" as a
    "User Prompt" to the `llm`, along with any existing chat history in the
    `llmchat` buffer.

Send a visual selection as context:
1. Select lines in any buffer using visual mode (e.g., `Vj`).
2. Call `:LLMChat`
>
    :'<,'>LLMChat
<
    The selected lines will be added to the `llmchat` buffer as a
    "Pasted Prompt". If you then call `:LLMChat` with an argument, or if
    the last prompt was a user prompt, the pasted content will be part of
    the context sent to `llm`.

Ranges can be used as well. To send whole file as context:
>
    :%LLMChat
<
Continue a conversation:
    Simply type your next prompt under the last "User Prompt" header in the
    `llmchat` buffer, then call `:LLMChat` (without arguments). The entire
    buffer content will be sent as context.  You are also free to
    modify any older context to help guide the conversation.

==============================================================================
5. Options                                              *llm-options*

`g:llm_timeout_seconds`                                 *g:llm_timeout_seconds*
    Default: 300 (5 minutes)
    This variable sets the timeout for `llm` jobs in seconds. If the `llm` job
    does not complete within this time, it will be automatically cancelled.

    Example:
>
    let g:llm_timeout_seconds = 600
<
    Sets the timeout to 10 minutes.

`g:llm_command`                                         *g:llm_command*
    Default: 'llm'
    This variable defines the command used to invoke the `llm` tool. You can
    set it to a specific path or a custom script if `llm` is not in your PATH
    or if you are using a wrapper.

    Example:
>
    let g:llm_command = '/usr/local/bin/llm'
<
    Sets the command to the absolute path of the `llm` executable.
>
    let g:llm_command = 'python3 -m llm'
<
    Sets the command to invoke `llm` via Python module.

`g:llm_enable_usage`                                    *g:llm_enable_usage*
    Default: v:false
    When set to v:true, the `-u` option will be added to the `llm` command.
    This enables `llm` to report token usage information as the last line of
    its output.

    Example:
>
    let g:llm_enable_usage = v:true
<
    Enables token usage reporting.

`g:llm_chat_new_behavior`                               *g:llm_chat_new_behavior*
    Default: 'current'
    This variable controls how the `llmchat` buffer is opened when
    `:LLMChat` is invoked.
    Possible values are:
    - 'vertical': Opens the `llmchat` buffer in a new vertical split window
                  to the right of the current window.
    - 'horizontal': Opens the `llmchat` buffer in a new horizontal split
                    window below the current window.
    - 'current': (Default) Opens the `llmchat` buffer in the current window.

    Example:
>
    let g:llm_chat_new_behavior = 'horizontal'
<
    Opens the `llmchat` buffer in a horizontal split.

    Tip: You can use CTRL-W | to make a vertical split full screen, CTRL-W _ to
    make a horizontal split full screen, and CTRL-W = to return all splits to
    original size.

`g:llm_model` (Default: `''`)
    This variable specifies the `llm` model to use with the `llm` command's
    `-m` option. If set to an empty string, the `-m` option will not be used,
    allowing `llm` to use its default model.

    >
        let g:llm_model = 'gpt-4o'
<

`g:llm_model_temperature`                               *g:llm_model_temperature*
    Default: `''`
    This variable specifies the temperature to use with the `llm` command's
    `-o temperature` option. If set to an empty string, the `-o temperature`
    option will not be used, allowing `llm` to use its default temperature.

    Example:
    >
        let g:llm_model_temperature = '0.4'
<

`g:llm_spinner_chars`                                   *g:llm_spinner_chars*
    Default: `['⠋', '⠙', '⠹', '⠸', '⠼', '⠴', '⠦', '⠧', '⠇', '⠏']`
    This variable defines the characters used for the loading spinner displayed
    in the `llmchat` buffer while an LLM job is running. You can customize the
    array with any Unicode characters or ASCII characters to change the spinner's
    appearance.

    Example:
    >
        let g:llm_spinner_chars = ['-', '\\', '|', '/']
<

==============================================================================
6. Notes                                                *llm-notes*

- If an `llm` job is already running when `:LLMChat` is called, the
  previous job will be cancelled before a new one starts.
- Error messages from the `llm` command will be displayed using `echom`.
- Fenced code in `llmchat` can be improved by setting
  `let g:markdown_fenced_languages = ['c', 'bash=sh', 'python]` as well
  as other languages you will work with. Markdown will look better if
  you have `set conceallevel=2.

vim:tw=78:ts=8:ft=help:norl:

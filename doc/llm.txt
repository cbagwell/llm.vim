*llm.txt*                                                               llm.vim

AI Assistant for Vim                                    *llm-plugin*

CONTENTS                                                *llm-contents*
  1. Introduction.......................|llm-introduction|
  2. Installation.......................|llm-installation|
  3. Keymaps............................|llm-keymaps|
  4. Commands...........................|llm-commands|
  5. Usage..............................|llm-usage|
  5. Options............................|llm-options|
  6. Notes..............................|llm-notes|

==============================================================================
1. Introduction                                         *llm-introduction*

llm.vim is a Vim plugin that integrates with Simon Willsion's `llm`
command-line tool to provide an AI chat assistant directly within Vim.
It manages a dedicated chat buffer, sends prompts to the `llm` tool, and
streams responses back into the buffer, allowing for an interactive
AI experience without laving your editor.

==============================================================================
2. Installation                                         *llm-installation*

This plugin requires the external `llm` command-line tool to be available in
your system's PATH and already configured.

Install using a plugin manager like `vim-plug`:
>
    Plug 'cbagwell/llm.vim'
<
Then run `:PlugInstall`.

==============================================================================
3. Keymaps                                              *llm-keymaps*

The following keymaps are active when in the `llmchat` buffer:

<CR>                                                    *<CR>*
    Sends the current prompt in the `llmchat` buffer to the `llm` tool,
    effectively calling `:LLMChat`.

<C-c>                                                   *<C-c>*
    Cancels any currently running `llm` job, equivalent to calling
    `:LLMCancel`.

[[ and ]]                                               *[[* *]]*
    By default, `[[` jumps to previous markdown header and `]]` jumps
    forward to next markdown header. In `llmchat.md`, this means you can jump
    between `Prompt` and `Response` headers. However, since responses may
    contain additional markdown headers, you may wish to update
    your `~/.vimrc` with more specific mappings to jump only between
    `User Prompt` and `Assistant Response` headers, like this example.
    Because the response from llm can be hard to read, you can use
    `[[` to quickly jump to last response header and then use `=]]`
    to re-ident and line wrap the response.

    augroup llmchat_install
        autocmd!
        " Jump backward and forward to '# >>>' or '# <<<' marker
        autocmd FileType markdown if expand('%:t') == 'llmchat.md' | nnoremap <silent><buffer> [[ :<C-U>call search('^#\\s\\+\\(>>>\\\\|<<<\).*', "bsW")<CR>| endif
        autocmd FileType markdown if expand('%:t') == 'llmchat.md' | nnoremap <silent><buffer> ]] :<C-U>call search('^#\\s\\+\\(>>>\\\\|<<<\).*', "sW")<CR>| endif
        autocmd FileType markdown if expand('%:t') == 'llmchat.md' | nnoremap <silent><buffer> = :set operatorfunc=llm#LLMReformatOperator<CR>g@| endif
        autocmd FileType markdown if expand('%:t') == 'llmchat.md' | onoremap <silent><buffer> <expr> = v:operator ==# '=' ? ':call llm#LLMReformatOperator("line")<CR>' : 'g@' | endif
        autocmd FileType markdown if expand('%:t') == 'llmchat.md' | vnoremap <silent><buffer> = :<C-U>call llm#LLMReformatOperator(visualmode())<CR>| endif
    augroup END
<

==============================================================================
4. Commands                                             *llm-commands*

                                                        *LLMChat*
:LLMChat [args]
:{range}LLMChat [args]
:{Visual}LLMChat [args]
    Opens or switches to the `llmchat` buffer and interacts with the `llm`
    tool.

    - If `[args]` are provided, they are treated as a new "User Prompt" and
      sent to the `llm` tool immediately.
    - If no `[args]` are provided:
        - If the last entry in the `llmchat` buffer is a "User Prompt",
          buffer content is sent to the `llm` tool.
        - Otherwise, a new empty "User Prompt" header is added, and the
          cursor is placed below it, allowing you to type your prompt before
	  re-issuing the command.
          You then call `:LLMChat` again (without arguments) to send it.
    - If a visual selection is active when `:LLMChat` is called,
      the selected lines are appended to the `llmchat` buffer under a
      Pasted Prompt header. This content is included in the context
      sent to the `llm` tool if a prompt is sent immediately after.

                                                        *LLMCancel*
:LLMCancel
    Cancels any currently running `llm` job.

                                                        *LLMIsRunning*
:LLMIsRunning
    Echoes "1" if an `llm` job is currently running, "0" otherwise.

                                                        *LLMFix*
:LLMFix
:{range}LLMFix
:{Visual}LLMFix
    Sends the current visual selection/range/line to LLM to fix any
    syntactical errors and replaces with the results.

                                                        *LLMFilter*
:LLMFilter
:{range}LLMFilter
:{Visual}LLMFilter
    Sends the current visual selection/range/line to `llm` to filter or
    transform it (similar to `%!llm "Fix grammer"`). The prompt given
    to `:LLMFilter` guides the transformation.
    For example: `:'<,'>LLMFilter Improve this text. Return only the original
    text with improvements.`  If modifying texting text then phrases
    such as "return original text with modifictions" and
    "do not return code blocks" can help.

                                                        *LLMComplete*
:LLMComplete
:{range}LLMComplete
:{Visual}LLMComplete
    Sends the current visual selection/range/line or optional arguments to
    `llm` to complete it. Can be used both for implement a function based on
    comment description or to complete incomplete sentences/paragraphs.

                                                        *LLMRead*
:LLMRead {prompt}
    Uses `llm` to send a prompt and add the response to the next line in
    the current buffer (similar to `:r !llm "Write a poem about LLMS"`).
    The function llm#LLMRead(prompt) can also be used to return a
    string. While in insert mode, you can use this syntax:
    `<C-r>=llm#LLMRead('Return a dogs name.  Just the name itself.')`

                                                        *LLMDoc*
:LLMDoc
    Uses `llm` to generate code documentation for the current range/visual
    selection. For example, visual select a function to document.

==============================================================================
5. Functions                                            *llm-functions*

llm#LLMRead({prompt})                                   *llm#LLMRead*
    Uses `llm` to read a prompt and return the response. This function is
    designed for use in expressions, for example, in insert mode using
    `<C-r>=llm#LLMRead('Return a dogs name.  Just the name itself.')` to
    insert the LLM's response directly into the buffer at current location.

llm#LLMReformatOperator({type})                        *llm#LLMReformatOperator*

    Meant to be used to define custom operators to reformat response text
    for improved readability.
>
        autocmd FileType markdown if expand('%:t') == 'llmchat.md' | nnoremap = :set operatorfunc=llm#LLMReformatOperator<CR>g@| endif
        autocmd FileType markdown if expand('%:t') == 'llmchat.md' | vnoremap <expr> = llm#LLMReformatOperator(visualmode())| endif
<

==============================================================================
4. Usage                                                *llm-usage*

Start a new chat or continue an existing one:
>
    :LLMChat
<
    This will open the `llmchat` buffer. If it's a new chat or the last
    response was from `llm`, it will add a new "User Prompt" header.
    You can then type your prompt and press `<CR>` (or move off the line)
    and call `:LLMChat` again to send it.

Send a direct prompt:
>
    :LLMChat What is the capital of France?
<
    This will immediately send "What is the capital of France?" as a
    "User Prompt" to the `llm`, along with any existing chat history in the
    `llmchat` buffer.

Send a visual selection as context:
1. Select lines in any buffer using visual mode (e.g., `Vj`).
2. Call `:LLMChat`
>
    :'<,'>LLMChat
<
    The selected lines will be added to the `llmchat` buffer as a
    "Pasted Prompt". If you then call `:LLMChat` with an argument, or if
    the last prompt was a user prompt, the pasted content will be part of
    the context sent to `llm`.

Ranges can be used as well. To send whole file as context:
>
    :%LLMChat
<
Continue a conversation:
    Simply type your next prompt under the last "User Prompt" header in the
    `llmchat` buffer, then call `:LLMChat` (without arguments). The entire
    buffer content will be sent as context.  You are also free to
    modify any older context to help guide the conversation.

==============================================================================
5. Options                                              *llm-options*

`g:llm_timeout_seconds`                                 *g:llm_timeout_seconds*
    Default: 300 (5 minutes)
    This variable sets the timeout for `llm` jobs in seconds. If the `llm` job
    does not complete within this time, it will be automatically cancelled.

    Example:
>
    let g:llm_timeout_seconds = 600
<
    Sets the timeout to 10 minutes.

`g:llm_command`                                         *g:llm_command*
    Default: 'llm'
    This variable defines the command used to invoke the `llm` tool. You can
    set it to a specific path or a custom script if `llm` is not in your PATH
    or if you are using a wrapper.

    Example:
>
    let g:llm_command = '/usr/local/bin/llm'
<
    Sets the command to the absolute path of the `llm` executable.
>
    let g:llm_command = 'python3 -m llm'
<
    Sets the command to invoke `llm` via Python module.

`g:llm_enable_usage`                                    *g:llm_enable_usage*
    Default: v:true 
    When set to v:true, token usage will be reported after each response.

    Example:
>
    let g:llm_enable_usage = v:false
<
`g:llm_chat_new_behavior`                               *g:llm_chat_new_behavior*
    Default: 'current'
    This variable controls how the `llmchat` buffer is opened when
    `:LLMChat` is invoked.
    Possible values are:
    - 'vertical': Opens the `llmchat` buffer in a new vertical split window
                  to the right of the current window.
    - 'horizontal': Opens the `llmchat` buffer in a new horizontal split
                    window below the current window.
    - 'current': (Default) Opens the `llmchat` buffer in the current window.

    Example:
>
    let g:llm_chat_new_behavior = 'horizontal'
<
    Opens the `llmchat` buffer in a horizontal split.

    Tip: You can use CTRL-W | to make a vertical split full screen, CTRL-W _ to
    make a horizontal split full screen, and CTRL-W = to return all splits to
    original size.

`g:llm_model` (Default: `''`)
    This variable specifies the `llm` model to use with the `llm` command's
    `-m` option. If set to an empty string, the `-m` option will not be used,
    allowing `llm` to use its default model.

    >
        let g:llm_model = 'gpt-4o'
<

`g:llm_model_temperature`                               *g:llm_model_temperature*
    Default: `''`
    This variable specifies the temperature to use with the `llm` command's
    `-o temperature` option. If set to an empty string, the `-o temperature`
    option will not be used, allowing `llm` to use its default temperature.

    Example:
    >
        let g:llm_model_temperature = '0.4'
<

`g:llm_spinner_chars`                                   *g:llm_spinner_chars*
    Default: `['⠋', '⠙', '⠹', '⠸', '⠼', '⠴', '⠦', '⠧', '⠇', '⠏']`
    This variable defines the characters used for the loading spinner displayed
    in the `llmchat` buffer while an LLM job is running. You can customize the
    array with any Unicode characters or ASCII characters to change the spinner's
    appearance.

    Example:
    >
        let g:llm_spinner_chars = ['-', '\\', '|', '/']
<

`g:llm_popup_notifications`                             *g:llm_popup_notifications*
    Default: `v:true`
    This variable controls whether popup notifications are displayed for `llm`
    status. Set to `v:false` to disable these popups.

    Example:
    >
        let g:llm_popup_notifications = v:false
<
    Disables popup notifications.

`g:llm_stream_reformat_response`                        *g:llm_stream_reformat_response*
    Default: `v:true`
    This variable controls whether streamed Assistant responses are
    automatically reformatted using `gqq` as they are appended to the
    `llmchat` buffer. When set to `v:true`, each line is rewrapped and
    indented. When set to `v:false`, no automatic reformatting occurs
    during streaming, and you can manually reformat sections using the
    `llm#LLMReformatOperator` (e.g., `gq` or `=` with appropriate keymaps).

    Example:
    >
        let g:llm_stream_reformat_response = v:false
<
    Disables automatic reformatting during streaming.

==============================================================================
6. Notes                                                *llm-notes*

- Error messages from the `llm` command will be displayed using `echom`.
- Fenced code in `llmchat` can be improved by setting
  `let g:markdown_fenced_languages = ['c', 'bash=sh', 'python]` as well
  as other languages you will work with. Markdown will look better if
  you have `set conceallevel=2.

vim:tw=78:ts=8:ft=help:norl:
